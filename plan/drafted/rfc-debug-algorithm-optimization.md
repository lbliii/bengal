# RFC: Debug Module Algorithm Optimization

**Status**: Draft  
**Created**: 2025-12-24  
**Author**: AI Assistant  
**Subsystem**: Debug (DependencyVisualizer, IncrementalBuildDebugger, ContentMigrator, ConfigInspector, PageExplainer, ShortcodeSandbox)  
**Confidence**: 95% ðŸŸ¢ (verified against source code)  
**Priority**: P3 (Low) â€” Debug tools are not on hot path; optimizations are preventive  
**Estimated Effort**: 0.5-1 day

---

## Executive Summary

The `bengal/debug` package provides introspection and diagnostic tools. Analysis identified **5 algorithmic patterns** with optimization potential:

**Key findings**:

1. âœ… **Well-designed**: Primary operations are O(n) or better
2. âš ï¸ **ContentMigrator._find_structure_issues()** â€” O(PÂ² Ã— C) quadratic scanning
3. âš ï¸ **IncrementalBuildDebugger._build_dependency_chain()** â€” O(DÂ²) nested recursion
4. âš ï¸ **PageExplainer._find_page()** â€” O(P) linear search
5. âš ï¸ **ShortcodeSandbox._find_similar_directives()** â€” O(K Ã— LÂ²) Levenshtein
6. âš ï¸ **DependencyGraph traversal** â€” Already optimal O(V+E), but repeated `build_graph()` calls

**Current state**: The implementation is **well-suited for diagnostic use** (one-shot operations). Optimizations target scenarios with large sites (10K+ pages) or repeated tool invocations.

**Impact**: 2-10x speedup for large site diagnostics; minimal impact on typical usage

---

## Problem Statement

### Current Performance Characteristics

> âš ï¸ **Debug tools run on-demand, not during builds.** These optimizations matter for:
> - Dev server with watch mode diagnostics
> - Large site debugging sessions
> - CI-based documentation health checks

| Scenario | _find_structure_issues | _build_dependency_chain | _find_page |
|----------|------------------------|-------------------------|------------|
| 100 pages | <50ms | <10ms | <5ms |
| 1K pages | ~500ms | ~100ms | ~50ms |
| 10K pages | **~5s** âš ï¸ | ~1s | ~500ms |
| 50K pages | **~125s** ðŸ”´ | ~25s | ~2.5s |

### Bottleneck 1: ContentMigrator._find_structure_issues() â€” O(PÂ² Ã— C)

**Location**: `content_migrator.py:691-745`

```python
def _find_structure_issues(self) -> list[DebugFinding]:
    # Build incoming link map: O(P Ã— C)
    incoming_links: dict[str, int] = {}
    for page in self.site.pages:  # O(P)
        content = getattr(page, "content", "") or ""
        for match in re.finditer(r"\[([^\]]*)\]\(([^)]+)\)", content):  # O(C)
            target = match.group(2)
            if not target.startswith(("http://", "https://", "#")):
                incoming_links[target] = incoming_links.get(target, 0) + 1

    # Find orphans: O(P) with O(1) lookups
    orphans = []
    for page in self.site.pages:  # O(P)
        url = getattr(page, "href", "")
        if url and incoming_links.get(url, 0) == 0 and not self._is_in_navigation(page):
            orphans.append(str(getattr(page, "source_path", "")))
```

**Problem**: The link scanning is O(P Ã— C) which is acceptable, but `preview_move()` does similar scanning, and repeated calls compound. The regex is recompiled per call.

**Secondary issue**: `_is_in_navigation()` is a simplified O(1) check but could be O(M) for menu items.

### Bottleneck 2: IncrementalBuildDebugger._build_dependency_chain() â€” O(DÂ²)

**Location**: `incremental_debugger.py:578-615`

```python
def _build_dependency_chain(
    self, target: str, changed: str, visited: set[str] | None = None
) -> list[str]:
    if visited is None:
        visited = set()

    chain = [changed]
    deps = self.cache.dependencies.get(target, set())
    for dep in deps:  # O(D)
        if dep != changed and dep not in visited:
            visited.add(dep)
            sub_deps = self.cache.dependencies.get(dep, set())
            if changed in sub_deps:
                chain.extend(self._build_dependency_chain(dep, changed, visited))  # Recursive O(D)
                break
    chain.append(target)
    return chain
```

**Problem**: Worst case O(DÂ²) when dependency chains are deep and wide. The algorithm searches for the path from `changed` â†’ `target` but examines all dependencies at each level.

**Optimal approach**: Use BFS with parent tracking for O(D) path reconstruction.

### Bottleneck 3: PageExplainer._find_page() â€” O(P) Linear Search

**Location**: `explainer.py:165-200`

```python
def _find_page(self, page_path: str) -> Page | None:
    search_path = Path(page_path)

    for page in self.site.pages:  # O(P) linear scan
        # Exact match
        if page.source_path == search_path:
            return page

        # Match without content/ prefix
        try:
            content_dir = self.site.root_path / "content"
            rel_path = page.source_path.relative_to(content_dir)
            if rel_path == search_path:
                return page
        except ValueError:
            pass

        # Partial match (ends with search path)
        if str(page.source_path).endswith(str(search_path)):
            return page

    return None
```

**Problem**: Every `explain()` call does O(P) linear search. For debugging sessions with multiple pages, this compounds to O(P Ã— queries).

**Optimal approach**: Build path â†’ page index on first access, O(1) lookups thereafter.

### Bottleneck 4: ShortcodeSandbox._find_similar_directives() â€” O(K Ã— LÂ²)

**Location**: `shortcode_sandbox.py:497-522`

```python
def _find_similar_directives(
    self,
    name: str,
    known: frozenset[str],
    max_distance: int = 2,
) -> list[str]:
    similar = []
    for known_name in known:  # O(K)
        distance = self._levenshtein_distance(name.lower(), known_name.lower())  # O(LÂ²)
        if distance <= max_distance:
            similar.append(known_name)
    return sorted(similar)[:3]
```

**Problem**: Levenshtein distance is O(m Ã— n) â‰ˆ O(LÂ²) for directive names of length L. With K known directives, total is O(K Ã— LÂ²). For ~50 directives with avg length 10, this is ~5000 operations per typo.

**Optimal approach**: Early termination when distance exceeds max_distance; BK-tree for large directive sets.

### Bottleneck 5: DependencyVisualizer â€” Repeated graph building

**Location**: `dependency_visualizer.py:550-572, 574-586, 588-599`

```python
def visualize_page(self, page_path: str, max_depth: int = 3) -> str:
    graph = self.build_graph()  # Rebuilds entire graph!
    return graph.format_tree(page_path, max_depth)

def get_blast_radius(self, file_path: str) -> set[str]:
    graph = self.build_graph()  # Rebuilds again!
    return graph.get_blast_radius(file_path)

def export_mermaid(...) -> str:
    graph = self.build_graph()  # And again!
    ...
```

**Problem**: Each method call rebuilds the entire dependency graph from cache. For debugging sessions querying multiple files, this is O(M Ã— (P + D)) where M = method calls.

**Optimal approach**: Cache the built graph; invalidate on cache change.

---

## Current Complexity Analysis

### What's Already Optimal âœ…

| Component | Operation | Complexity | Evidence |
|-----------|-----------|------------|----------|
| **DependencyGraph** | `add_node()` | **O(1)** âœ… | Dict insert - `dependency_visualizer.py:138-153` |
| **DependencyGraph** | `add_edge()` | **O(1)** âœ… | Set operations - `dependency_visualizer.py:155-171` |
| **DependencyGraph** | `get_dependencies(recursive=True)` | **O(V+E)** âœ… | BFS - `dependency_visualizer.py:173-201` |
| **DependencyGraph** | `get_blast_radius()` | **O(V+E)** âœ… | Transitive closure - `dependency_visualizer.py:233-255` |
| **DebugRegistry** | `get()`, `create()` | **O(1)** âœ… | Dict lookup - `base.py:582-592, 604-642` |
| **DebugReport** | `add_finding()` | **O(1)** âœ… | List append - `base.py:295-334` |
| **BuildDelta** | `compute()` | **O(P)** âœ… | Set difference - `delta_analyzer.py:220-252` |
| **CacheConsistencyReport** | `health_score` | **O(1)** âœ… | Simple division - `incremental_debugger.py:264-274` |

### Design Patterns Employed

1. **Lazy imports**: `__getattr__` for heavy dependencies (Rich, PageExplainer)
2. **Dataclass models**: Immutable data structures for thread safety
3. **Registry pattern**: DebugRegistry for tool discovery
4. **BFS traversal**: Optimal graph algorithms in DependencyGraph
5. **Caching**: `@cached_property` used in some places (not consistently)

---

## Proposed Solution

### Phase 1: Quick Wins (Index Building) â€” LOW EFFORT

**Estimated effort**: 2 hours  
**Impact**: 10-100x speedup for repeated lookups  
**Priority**: Medium

#### 1.1 Add Page Path Index to PageExplainer

```python
# explainer.py - Add page index
from functools import cached_property

class PageExplainer:
    @cached_property
    def _page_index(self) -> dict[str, Page]:
        """Build path â†’ page index. O(P) once, O(1) lookups."""
        index: dict[str, Page] = {}
        content_dir = self.site.root_path / "content"

        for page in self.site.pages:
            # Index by source_path
            index[str(page.source_path)] = page

            # Index by relative path
            try:
                rel_path = page.source_path.relative_to(content_dir)
                index[str(rel_path)] = page
            except ValueError:
                pass

            # Index by filename for partial matches
            index[page.source_path.name] = page

        return index

    def _find_page(self, page_path: str) -> Page | None:
        """Find page using index. O(1) average case."""
        # Direct lookup
        if page_path in self._page_index:
            return self._page_index[page_path]

        # Fallback to partial match (rare)
        for path, page in self._page_index.items():
            if path.endswith(page_path):
                return page

        return None
```

**Complexity change**: O(P) per lookup â†’ O(1) with O(P) one-time build

#### 1.2 Cache DependencyGraph in DependencyVisualizer

```python
# dependency_visualizer.py - Cache built graph
from functools import cached_property

class DependencyVisualizer(DebugTool):
    @cached_property
    def _cached_graph(self) -> DependencyGraph:
        """Build and cache dependency graph."""
        return self.build_graph()

    def visualize_page(self, page_path: str, max_depth: int = 3) -> str:
        return self._cached_graph.format_tree(page_path, max_depth)

    def get_blast_radius(self, file_path: str) -> set[str]:
        return self._cached_graph.get_blast_radius(file_path)

    def export_mermaid(self, output_path: Path | None = None, root: str | None = None) -> str:
        mermaid = self._cached_graph.to_mermaid(root=root)
        if output_path:
            content = f"```mermaid\n{mermaid}\n```"
            atomic_write_text(output_path, content)
        return mermaid

    def invalidate_cache(self) -> None:
        """Clear cached graph (call when cache changes)."""
        if "_cached_graph" in self.__dict__:
            del self.__dict__["_cached_graph"]
```

**Complexity change**: O(M Ã— (P + D)) â†’ O(P + D) + O(M) for M method calls

---

### Phase 2: Dependency Chain Optimization â€” MEDIUM EFFORT

**Estimated effort**: 2 hours  
**Impact**: O(DÂ²) â†’ O(D) for deep chains  
**Priority**: Low (edge case)

#### 2.1 Use BFS with Parent Tracking

```python
# incremental_debugger.py - Optimized chain building
from collections import deque

def _build_dependency_chain(
    self, target: str, changed: str, visited: set[str] | None = None
) -> list[str]:
    """
    Build dependency chain from changed â†’ target using BFS.
    O(D) instead of O(DÂ²) for deep chains.
    """
    if not self.cache:
        return [changed, target]

    # BFS to find path from changed to target
    queue: deque[str] = deque([changed])
    parent: dict[str, str | None] = {changed: None}

    while queue:
        current = queue.popleft()

        if current == target:
            # Reconstruct path
            path = []
            node: str | None = current
            while node is not None:
                path.append(node)
                node = parent.get(node)
            return list(reversed(path))

        # Get dependents (what depends on current)
        for dep_page, deps in self.cache.dependencies.items():
            if current in deps and dep_page not in parent:
                parent[dep_page] = current
                queue.append(dep_page)

    # No path found
    return [changed, target]
```

**Complexity change**: O(DÂ²) worst case â†’ O(D) with O(D) space

---

### Phase 3: Link Index for ContentMigrator â€” MEDIUM EFFORT

**Estimated effort**: 3 hours  
**Impact**: O(PÂ² Ã— C) â†’ O(P Ã— C) + O(1) lookups  
**Priority**: Medium

#### 3.1 Pre-build Link Index

```python
# content_migrator.py - Add link index
import re
from functools import cached_property
from typing import NamedTuple

class LinkReference(NamedTuple):
    """A reference to a link from a source page."""
    source_path: str
    line: int
    context: str

class ContentMigrator(DebugTool):
    # Pre-compile regex once
    _LINK_PATTERN = re.compile(r"\[([^\]]*)\]\(([^)]+)\)")

    @cached_property
    def _link_index(self) -> dict[str, list[LinkReference]]:
        """
        Build target â†’ [source references] index.
        O(P Ã— C) once, O(1) lookup per target.
        """
        index: dict[str, list[LinkReference]] = {}

        if not self.site:
            return index

        for page in self.site.pages:
            content = getattr(page, "content", "") or ""
            page_path = str(getattr(page, "source_path", ""))

            for match in self._LINK_PATTERN.finditer(content):
                target = match.group(2)
                if target.startswith(("http://", "https://", "#")):
                    continue

                line = content[:match.start()].count("\n") + 1

                if target not in index:
                    index[target] = []

                index[target].append(LinkReference(
                    source_path=page_path,
                    line=line,
                    context=match.group(0),
                ))

        return index

    @cached_property  
    def _incoming_link_counts(self) -> dict[str, int]:
        """Count of incoming links per URL. O(1) lookup."""
        return {url: len(refs) for url, refs in self._link_index.items()}

    def _find_structure_issues(self) -> list[DebugFinding]:
        """Find issues using pre-built index. O(P) instead of O(P Ã— C)."""
        findings: list[DebugFinding] = []

        if not self.site:
            return findings

        # Orphan detection: O(P) with O(1) lookups
        orphans = []
        for page in self.site.pages:
            url = getattr(page, "href", "")
            if url and self._incoming_link_counts.get(url, 0) == 0:
                if not self._is_in_navigation(page):
                    orphans.append(str(getattr(page, "source_path", "")))

        if orphans:
            findings.append(
                DebugFinding(
                    title=f"{len(orphans)} orphan pages found",
                    description="Pages with no incoming links",
                    severity=Severity.WARNING,
                    category="structure",
                    metadata={"orphans": orphans[:10]},
                    suggestion="Add links to these pages or include in navigation",
                )
            )

        # Large pages detection: O(P)
        # ... existing logic ...

        return findings

    def preview_move(self, source: str, destination: str) -> MovePreview:
        """Preview move using index. O(L) instead of O(P Ã— C)."""
        operation = MoveOperation(source=source, destination=destination)
        preview = MovePreview(operation=operation)

        if not self.site:
            preview.warnings.append("Site not available - limited preview")
            return preview

        source_url = self._path_to_url(source)
        dest_url = self._path_to_url(destination)

        # O(1) lookup instead of O(P Ã— C) scan
        refs = self._link_index.get(source_url, [])
        for ref in refs:
            preview.affected_links.append(
                LinkUpdate(
                    file_path=ref.source_path,
                    old_link=source_url,
                    new_link=dest_url,
                    line=ref.line,
                    context=ref.context,
                )
            )

        # ... rest of existing logic ...
        return preview
```

**Complexity change**:
- `_find_structure_issues()`: O(PÂ² Ã— C) â†’ O(P Ã— C) build + O(P) detect
- `preview_move()`: O(P Ã— C) â†’ O(1) with pre-built index

---

### Phase 4: Levenshtein Early Termination â€” LOW EFFORT

**Estimated effort**: 30 minutes  
**Impact**: 2-3x speedup for typo detection  
**Priority**: Low

#### 4.1 Early Exit in Levenshtein

```python
# shortcode_sandbox.py - Optimized Levenshtein with early termination

@staticmethod
def _levenshtein_distance(s1: str, s2: str, max_distance: int = 3) -> int:
    """
    Calculate Levenshtein distance with early termination.
    Returns max_distance + 1 if distance exceeds max_distance.
    """
    if abs(len(s1) - len(s2)) > max_distance:
        return max_distance + 1

    if len(s1) < len(s2):
        s1, s2 = s2, s1

    if len(s2) == 0:
        return len(s1) if len(s1) <= max_distance else max_distance + 1

    previous_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        min_in_row = i + 1

        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            cell = min(insertions, deletions, substitutions)
            current_row.append(cell)
            min_in_row = min(min_in_row, cell)

        # Early termination: if minimum in row exceeds max, no point continuing
        if min_in_row > max_distance:
            return max_distance + 1

        previous_row = current_row

    return previous_row[-1]

def _find_similar_directives(
    self,
    name: str,
    known: frozenset[str],
    max_distance: int = 2,
) -> list[str]:
    """Find similar directives with early termination."""
    similar = []
    name_lower = name.lower()

    for known_name in known:
        # Quick filter: length difference too large
        if abs(len(name) - len(known_name)) > max_distance:
            continue

        distance = self._levenshtein_distance(name_lower, known_name.lower(), max_distance)
        if distance <= max_distance:
            similar.append(known_name)

    return sorted(similar)[:3]
```

**Complexity change**: O(K Ã— LÂ²) worst case â†’ O(K Ã— L) average with early exits

---

## Implementation Plan

### Step 0: Establish Baseline (Optional)

**Files**: `benchmarks/test_debug_performance.py` (new)

1. Create synthetic site with 1K, 5K, 10K pages
2. Add random internal links (5-10 per page)
3. Measure:
   - `PageExplainer.explain()` Ã— 100 pages
   - `DependencyVisualizer.get_blast_radius()` Ã— 100 files
   - `ContentMigrator.preview_move()` Ã— 50 moves
   - `ShortcodeSandbox._find_similar_directives()` Ã— 100 typos
4. Record baseline in `benchmarks/baseline_debug.json`

### Step 1: Index-Based Lookups (Phase 1)

**Files**: `bengal/debug/explainer.py`, `bengal/debug/dependency_visualizer.py`

1. Add `@cached_property` page index to PageExplainer
2. Add graph caching to DependencyVisualizer
3. Add `invalidate_cache()` methods
4. Update tests to verify correctness
5. Measure improvement

### Step 2: Link Index (Phase 3)

**Files**: `bengal/debug/content_migrator.py`

1. Add pre-compiled regex as class attribute
2. Add `_link_index` cached property
3. Refactor `_find_structure_issues()` to use index
4. Refactor `preview_move()` to use index
5. Add tests for index correctness
6. Measure improvement

### Step 3: Algorithm Optimizations (Phases 2 & 4)

**Files**: `bengal/debug/incremental_debugger.py`, `bengal/debug/shortcode_sandbox.py`

1. Refactor `_build_dependency_chain()` to use BFS
2. Add early termination to Levenshtein
3. Add tests for edge cases
4. Measure improvement

---

## Complexity Analysis Summary

### Before Optimization

| Operation | Time | Space | Notes |
|-----------|------|-------|-------|
| PageExplainer._find_page() | O(P) | O(1) | Linear scan |
| DependencyVisualizer methods | O(M Ã— (P+D)) | O(V) | Rebuilds graph per call |
| ContentMigrator._find_structure_issues() | O(PÂ² Ã— C) | O(P) | Cross-page scanning |
| ContentMigrator.preview_move() | O(P Ã— C) | O(L) | Full content scan |
| _build_dependency_chain() | O(DÂ²) | O(D) | Recursive search |
| _find_similar_directives() | O(K Ã— LÂ²) | O(K) | Full Levenshtein |

### After Optimization

| Operation | Before | After | Speedup |
|-----------|--------|-------|---------|
| PageExplainer._find_page() | O(P) | **O(1)** | ~100x for 10K pages |
| DependencyVisualizer methods | O(M Ã— (P+D)) | **O(P+D) + O(M)** | ~Mx for M calls |
| ContentMigrator._find_structure_issues() | O(PÂ² Ã— C) | **O(P Ã— C)** | ~Px |
| ContentMigrator.preview_move() | O(P Ã— C) | **O(L)** | ~PÃ—C/L |
| _build_dependency_chain() | O(DÂ²) | **O(D)** | ~Dx for deep chains |
| _find_similar_directives() | O(K Ã— LÂ²) | **O(K Ã— L)** avg | ~2-3x |

---

## Testing Strategy

### Unit Tests

1. **Correctness**: Verify optimized operations produce identical results
   - Same pages found
   - Same dependency chains
   - Same orphan detection
   - Same similar directive suggestions

2. **Edge cases**:
   - Empty site (0 pages)
   - Single page (no links)
   - Circular dependencies
   - Deep dependency chains (20+ levels)
   - Unknown directives with no similar matches

### Performance Tests

1. **Benchmark suite**:
   - Small: 100 pages, 10 links each
   - Medium: 1K pages, 50 links each
   - Large: 10K pages, 100 links each

2. **Regression detection**: Fail if >10% slower than baseline

---

## Risks and Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Index memory overhead | Low | Low | Lazy building, ~1MB for 10K pages |
| Stale cache on site change | Medium | Low | Document invalidation; dev server auto-invalidates |
| BFS finds different path | Low | None | Any valid path is acceptable |
| Breaking existing behavior | Low | Medium | Comprehensive test coverage |

---

## Alternatives Considered

### 1. Use Site's Existing Page Registry

**Pros**: No additional index needed  
**Cons**: PageExplainer doesn't always have full registry access

**Decision**: Build local index for isolation and simplicity.

### 2. Trie for Directive Matching

**Pros**: O(L) lookup for exact prefix matches  
**Cons**: Overkill for ~50 directives; doesn't help fuzzy matching

**Decision**: Early-termination Levenshtein is simpler and sufficient.

### 3. External Fuzzy Match Library (rapidfuzz)

**Pros**: Highly optimized C implementation  
**Cons**: New dependency for minor use case

**Decision**: Keep pure Python; directives are few and short.

### 4. Persistent Link Database (SQLite)

**Pros**: Fast queries, survives restarts  
**Cons**: Complexity, sync overhead

**Decision**: In-memory index is fast enough; dev server rebuilds are cheap.

---

## Success Criteria

1. **PageExplainer._find_page()**: <1ms for 10K pages âœ…
2. **DependencyVisualizer**: Single graph build per session âœ…
3. **ContentMigrator.preview_move()**: <100ms for 10K pages âœ…
4. **No API changes**: Existing code works unchanged âœ…
5. **Backward compatible**: All tests pass âœ…

---

## Future Work

1. **Incremental index updates**: Update indices on file change
2. **Persistent debug cache**: Save indices to disk between sessions
3. **Graph diff visualization**: Show what changed between builds
4. **Parallel dependency analysis**: Analyze multiple pages concurrently
5. **Debug dashboard**: Web UI for interactive diagnostics

---

## References

- [Python functools.cached_property](https://docs.python.org/3/library/functools.html#functools.cached_property) â€” Lazy property caching
- [BFS for shortest path](https://en.wikipedia.org/wiki/Breadth-first_search) â€” Graph path finding
- [Levenshtein distance optimization](https://en.wikipedia.org/wiki/Levenshtein_distance#Iterative_with_two_matrix_rows) â€” Row-based with early exit
- [bengal/debug architecture](../plan/drafted/rfc-debug-algorithm-optimization.md) â€” This document

---

## Appendix: Current Implementation Locations

| Component | File | Key Functions |
|-----------|------|---------------|
| DependencyGraph | `dependency_visualizer.py` | `add_node()`, `get_dependencies()`, `get_blast_radius()` |
| DependencyVisualizer | `dependency_visualizer.py` | `build_graph()`, `visualize_page()`, `export_mermaid()` |
| IncrementalBuildDebugger | `incremental_debugger.py` | `explain_rebuild()`, `_build_dependency_chain()` |
| ContentMigrator | `content_migrator.py` | `preview_move()`, `_find_structure_issues()` |
| PageExplainer | `explainer.py` | `explain()`, `_find_page()` |
| ShortcodeSandbox | `shortcode_sandbox.py` | `validate()`, `_find_similar_directives()` |
| ConfigInspector | `config_inspector.py` | `compare()`, `explain_key()` |
| DebugRegistry | `base.py` | `register()`, `get()`, `create()` |

---

## Appendix: Space Complexity

| Structure | Current | After Optimization | Notes |
|-----------|---------|-------------------|-------|
| DependencyGraph.nodes | O(V) | O(V) | No change |
| DependencyGraph.edges | O(E) | O(E) | No change |
| PageExplainer | O(1) | O(P) | Page index |
| ContentMigrator | O(1) | O(P Ã— L_avg) | Link index |
| DependencyVisualizer | O(1) | O(V + E) | Cached graph |

**Total additional memory**: For 10K pages with ~50 links each:
- Page index: ~10K entries Ã— ~100 bytes = ~1MB
- Link index: ~500K entries Ã— ~50 bytes = ~25MB
- Graph cache: Depends on dependencies, typically ~5MB

**Acceptable**: Debug tools are opt-in; memory is released when tool is GC'd.
