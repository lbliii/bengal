# RFC: Utility Extraction and Consolidation

```yaml
Title: Utility Extraction and Consolidation
Author: AI (Claude) + Human Review
Date: 2025-12-08
Status: Accepted
Confidence: 92%
```

---

## Problem Statement

### Current State

Bengal's codebase contains **repeated patterns** scattered across multiple modules that implement the same logic with slight variations. Analysis identified:

- **12+ files** implementing SHA256 hashing with truncation
- **10 files** creating ad-hoc `threading.local()` instances
- **3 files** implementing exponential backoff retry logic
- **43 files** with 81 occurrences of `mkdir(parents=True, exist_ok=True)`
- **2 files** with identical inline markdown rendering code

**Evidence**:
- `cache/build_cache/file_tracking.py:59` - `hashlib.sha256()` file hashing
- `cache/build_cache/parsed_content_cache.py:82` - `hashlib.sha256(metadata_str.encode()).hexdigest()`
- `orchestration/render.py:74` - `_thread_local = threading.local()`
- `core/site/core.py:58` - `_thread_local = threading.local()`
- `utils/file_lock.py:96-127` - exponential backoff implementation
- `health/linkcheck/async_checker.py:301-318` - `_calculate_backoff()` with jitter

### Pain Points

1. **Maintenance burden**: Bug fixes or improvements must be applied to multiple locations
2. **Inconsistent behavior**: Different implementations may have subtle variations (e.g., some hashes truncate to 8 chars, others to 16)
3. **Code duplication**: ~150 lines of duplicated logic across the codebase
4. **Testing overhead**: Same functionality tested in multiple places or undertested
5. **Onboarding friction**: New contributors must discover multiple implementations

### User Impact

- **Developers**: Must know which file to look in for the "canonical" implementation
- **Contributors**: Risk introducing bugs by copying wrong implementation
- **Maintainers**: Must update multiple files for what should be single-point changes

---

## Goals & Non-Goals

### Goals

1. **Consolidate hashing utilities** into `bengal/utils/hashing.py` with clear, documented functions
2. **Extract retry/backoff logic** into reusable `bengal/utils/retry.py` with both sync and async variants
3. **Standardize thread-local patterns** with a factory class in `bengal/utils/thread_local.py`
4. **Maintain backward compatibility** â€” no breaking changes to public APIs
5. **Improve testability** â€” centralized utilities are easier to unit test

### Non-Goals

- **Not refactoring all directory creation** â€” 81 occurrences is too invasive for limited benefit
- **Not creating abstractions for single-use patterns** â€” only extract patterns with 3+ uses
- **Not changing external interfaces** â€” CLI, config, and public APIs remain unchanged
- **Not introducing new dependencies** â€” utilities use stdlib only

---

## Architecture Impact

### Affected Subsystems

- **Utils** (`bengal/utils/`): Primary target â€” adding 3 new modules
  - New: `hashing.py`, `retry.py`, `thread_local.py`
  
- **Cache** (`bengal/cache/`): Will import from `utils/hashing.py`
  - `build_cache/file_tracking.py`
  - `build_cache/parsed_content_cache.py`
  - `build_cache/rendered_output_cache.py`
  - `build_cache/fingerprint.py`
  - `query_index.py`
  
- **Orchestration** (`bengal/orchestration/`): Will import from `utils/thread_local.py`
  - `render.py`
  - `menu.py`
  
- **Core** (`bengal/core/`): Will import from `utils/hashing.py` and `utils/thread_local.py`
  - `site/core.py`
  - `asset.py`
  
- **Rendering** (`bengal/rendering/`): Will import from `utils/thread_local.py`
  - `pipeline/thread_local.py` (becomes thin wrapper)
  - `template_profiler.py`
  - `pygments_cache.py`
  
- **Health** (`bengal/health/`): Will import from `utils/retry.py`
  - `linkcheck/async_checker.py`
  
- **Server** (`bengal/server/`): Will import from `utils/thread_local.py`
  - `request_handler.py`
  - `build_handler.py`
  - `resource_manager.py`
  - `reload_controller.py`

### Integration Points

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     bengal/utils/                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  hashing.py  â”‚  retry.py    â”‚   thread_local.py         â”‚
â”‚              â”‚              â”‚                           â”‚
â”‚ â€¢ hash_str   â”‚ â€¢ retry()    â”‚ â€¢ ThreadLocalCache[T]     â”‚
â”‚ â€¢ hash_dict  â”‚ â€¢ async_retryâ”‚ â€¢ get_or_create()         â”‚
â”‚ â€¢ hash_file  â”‚ â€¢ backoff()  â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚                     â”‚
       â–¼              â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    cache/    â”‚ â”‚   health/  â”‚  â”‚ orchestration/ rendering/â”‚
â”‚              â”‚ â”‚            â”‚  â”‚ server/ core/            â”‚
â”‚ â€¢ fingerprintâ”‚ â”‚ â€¢ linkcheckâ”‚  â”‚ â€¢ render pipeline        â”‚
â”‚ â€¢ file_track â”‚ â”‚            â”‚  â”‚ â€¢ template profiler      â”‚
â”‚ â€¢ query_indexâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ pygments cache         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Design Options

### Option A: Standalone Utility Modules (Recommended)

**Description**: Create three new independent utility modules in `bengal/utils/` with clear, single-responsibility functions.

**Pros**:
- Simple, flat structure
- Easy to import and use
- No new abstractions to learn
- Minimal risk of over-engineering
- Each module can be adopted incrementally

**Cons**:
- Slightly more imports in consuming files
- No enforced usage pattern (developers could still create ad-hoc implementations)

**Complexity**: Simple

**Evidence**: Pattern successfully used in existing `utils/text.py`, `utils/file_io.py`

---

### Option B: Utility Classes with Registry

**Description**: Create utility classes with a central registry for discovery and configuration.

**Pros**:
- Centralized configuration
- Discoverable via registry
- Could support plugins

**Cons**:
- Over-engineered for current needs
- More complex API
- Registry adds indirection
- Against Bengal's "no magic" principle

**Complexity**: Complex

---

### Option C: Keep Domain-Specific Implementations

**Description**: Keep current pattern of domain-specific implementations, but improve documentation.

**Pros**:
- No code changes required
- Each module is self-contained

**Cons**:
- Doesn't solve the maintenance burden
- Inconsistencies will persist
- Code duplication continues

**Complexity**: N/A (status quo)

---

### Recommended: Option A

**Reasoning**: Standalone modules follow Bengal's existing patterns (`utils/text.py`, `utils/file_io.py`), maintain simplicity, and allow incremental adoption. The utilities are stateless functions, which aligns with Bengal's preference for explicit state management and composition over inheritance.

---

## Detailed Design (Option A)

### New Module: `bengal/utils/hashing.py`

```python
"""
Cryptographic hashing utilities for Bengal.

Provides standardized hashing for file fingerprinting, cache keys,
and content-addressable storage.

Example:
    from bengal.utils.hashing import hash_str, hash_file, hash_dict
    
    # Hash string content
    key = hash_str("hello world")  # "b94d27b9..."
    
    # Hash with truncation (for fingerprints)
    fingerprint = hash_str("hello world", truncate=8)  # "b94d27b9"
    
    # Hash file content
    file_hash = hash_file(Path("content/post.md"))
    
    # Hash dict deterministically
    config_hash = hash_dict({"key": "value", "nested": [1, 2, 3]})
"""

from __future__ import annotations

import hashlib
import json
from pathlib import Path
from typing import Any


def hash_str(content: str, truncate: int | None = None, algorithm: str = "sha256") -> str:
    """
    Hash string content using specified algorithm.
    
    Args:
        content: String content to hash
        truncate: Truncate result to N characters (None = full hash)
        algorithm: Hash algorithm ('sha256', 'md5')
    
    Returns:
        Hex digest of hash, optionally truncated
    
    Examples:
        >>> hash_str("hello")
        '2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824'
        >>> hash_str("hello", truncate=16)
        '2cf24dba5fb0a30e'
    """
    hasher = hashlib.new(algorithm)
    hasher.update(content.encode("utf-8"))
    digest = hasher.hexdigest()
    return digest[:truncate] if truncate else digest


def hash_bytes(content: bytes, truncate: int | None = None, algorithm: str = "sha256") -> str:
    """
    Hash bytes content using specified algorithm.
    
    Args:
        content: Bytes content to hash
        truncate: Truncate result to N characters (None = full hash)
        algorithm: Hash algorithm ('sha256', 'md5')
    
    Returns:
        Hex digest of hash, optionally truncated
    """
    hasher = hashlib.new(algorithm)
    hasher.update(content)
    digest = hasher.hexdigest()
    return digest[:truncate] if truncate else digest


def hash_dict(
    data: dict[str, Any],
    truncate: int | None = 16,
    algorithm: str = "sha256",
) -> str:
    """
    Hash dictionary deterministically (sorted keys, string serialization).
    
    Args:
        data: Dictionary to hash
        truncate: Truncate result to N characters (default: 16)
        algorithm: Hash algorithm ('sha256', 'md5')
    
    Returns:
        Hex digest of hash
    
    Examples:
        >>> hash_dict({"b": 2, "a": 1})
        '...'  # Same as hash_dict({"a": 1, "b": 2})
    """
    # Deterministic serialization: sort keys, use default=str for non-JSON types
    serialized = json.dumps(data, sort_keys=True, default=str)
    return hash_str(serialized, truncate=truncate, algorithm=algorithm)


def hash_file(
    path: Path,
    truncate: int | None = None,
    algorithm: str = "sha256",
    chunk_size: int = 8192,
) -> str:
    """
    Hash file content by streaming (memory-efficient for large files).
    
    Args:
        path: Path to file
        truncate: Truncate result to N characters (None = full hash)
        algorithm: Hash algorithm ('sha256', 'md5')
        chunk_size: Read buffer size in bytes
    
    Returns:
        Hex digest of file content hash
    
    Raises:
        FileNotFoundError: If file doesn't exist
    """
    hasher = hashlib.new(algorithm)
    
    with open(path, "rb") as f:
        while chunk := f.read(chunk_size):
            hasher.update(chunk)
    
    digest = hasher.hexdigest()
    return digest[:truncate] if truncate else digest


def hash_file_with_stat(
    path: Path,
    truncate: int | None = 8,
    algorithm: str = "sha256",
) -> str:
    """
    Hash file for fingerprinting (includes mtime for fast invalidation).
    
    Combines file content hash with modification time for efficient
    cache invalidation without re-hashing unchanged files.
    
    Args:
        path: Path to file
        truncate: Truncate result to N characters (default: 8 for URLs)
        algorithm: Hash algorithm
    
    Returns:
        Fingerprint string suitable for URLs
    """
    stat = path.stat()
    content_hash = hash_file(path, algorithm=algorithm)
    # Combine with mtime for fast invalidation
    combined = f"{content_hash}:{stat.st_mtime_ns}"
    return hash_str(combined, truncate=truncate, algorithm=algorithm)
```

---

### New Module: `bengal/utils/retry.py`

```python
"""
Retry utilities with exponential backoff.

Provides both synchronous and asynchronous retry decorators and functions
with configurable backoff strategies and jitter.

Example:
    from bengal.utils.retry import retry_with_backoff, calculate_backoff
    
    # Retry function with backoff
    result = retry_with_backoff(
        fetch_data,
        retries=3,
        base_delay=0.5,
        exceptions=(ConnectionError, TimeoutError),
    )
    
    # Calculate backoff for custom retry loop
    delay = calculate_backoff(attempt=2, base=0.5, max_delay=10.0)
"""

from __future__ import annotations

import asyncio
import random
import time
from collections.abc import Awaitable, Callable
from typing import Any, TypeVar

T = TypeVar("T")


def calculate_backoff(
    attempt: int,
    base: float = 0.5,
    max_delay: float = 10.0,
    jitter: bool = True,
) -> float:
    """
    Calculate exponential backoff delay with optional jitter.
    
    Uses formula: base * (2 ^ attempt) with Â±25% jitter.
    
    Args:
        attempt: Current attempt number (0-indexed)
        base: Base delay in seconds
        max_delay: Maximum delay cap
        jitter: Add random jitter to prevent thundering herd
    
    Returns:
        Delay in seconds
    
    Examples:
        >>> calculate_backoff(0, base=0.5)  # ~0.5s
        >>> calculate_backoff(1, base=0.5)  # ~1.0s
        >>> calculate_backoff(2, base=0.5)  # ~2.0s
    """
    delay = base * (2 ** attempt)
    delay = min(delay, max_delay)
    
    if jitter:
        # Â±25% jitter
        jitter_range = delay * 0.25
        delay += random.uniform(-jitter_range, jitter_range)
    
    return max(0.1, delay)  # Minimum 100ms


def retry_with_backoff(
    func: Callable[[], T],
    retries: int = 3,
    base_delay: float = 0.5,
    max_delay: float = 10.0,
    jitter: bool = True,
    exceptions: tuple[type[Exception], ...] = (Exception,),
    on_retry: Callable[[int, Exception], None] | None = None,
) -> T:
    """
    Execute function with retry and exponential backoff.
    
    Args:
        func: Function to execute (no arguments)
        retries: Maximum retry attempts
        base_delay: Base delay between retries
        max_delay: Maximum delay cap
        jitter: Add jitter to prevent thundering herd
        exceptions: Exception types to catch and retry
        on_retry: Optional callback(attempt, exception) on each retry
    
    Returns:
        Result of successful function call
    
    Raises:
        Last exception if all retries exhausted
    
    Example:
        >>> result = retry_with_backoff(
        ...     lambda: requests.get(url),
        ...     retries=3,
        ...     exceptions=(ConnectionError,),
        ... )
    """
    last_error: Exception | None = None
    
    for attempt in range(retries + 1):
        try:
            return func()
        except exceptions as e:
            last_error = e
            
            if attempt < retries:
                delay = calculate_backoff(attempt, base_delay, max_delay, jitter)
                
                if on_retry:
                    on_retry(attempt, e)
                
                time.sleep(delay)
            else:
                raise
    
    # Should never reach here, but satisfies type checker
    raise last_error  # type: ignore


async def async_retry_with_backoff(
    coro_func: Callable[[], Awaitable[T]],
    retries: int = 3,
    base_delay: float = 0.5,
    max_delay: float = 10.0,
    jitter: bool = True,
    exceptions: tuple[type[Exception], ...] = (Exception,),
    on_retry: Callable[[int, Exception], None] | None = None,
) -> T:
    """
    Execute async function with retry and exponential backoff.
    
    Args:
        coro_func: Async function to execute (no arguments, returns awaitable)
        retries: Maximum retry attempts
        base_delay: Base delay between retries
        max_delay: Maximum delay cap
        jitter: Add jitter to prevent thundering herd
        exceptions: Exception types to catch and retry
        on_retry: Optional callback(attempt, exception) on each retry
    
    Returns:
        Result of successful coroutine
    
    Raises:
        Last exception if all retries exhausted
    
    Example:
        >>> result = await async_retry_with_backoff(
        ...     lambda: client.get(url),
        ...     retries=3,
        ...     exceptions=(httpx.TimeoutException,),
        ... )
    """
    last_error: Exception | None = None
    
    for attempt in range(retries + 1):
        try:
            return await coro_func()
        except exceptions as e:
            last_error = e
            
            if attempt < retries:
                delay = calculate_backoff(attempt, base_delay, max_delay, jitter)
                
                if on_retry:
                    on_retry(attempt, e)
                
                await asyncio.sleep(delay)
            else:
                raise
    
    raise last_error  # type: ignore
```

---

### New Module: `bengal/utils/thread_local.py`

```python
"""
Thread-local caching utilities.

Provides a generic thread-local cache for expensive-to-create objects
like parsers, database connections, or pipeline instances.

Example:
    from bengal.utils.thread_local import ThreadLocalCache
    
    # Create a cache for markdown parsers
    parser_cache = ThreadLocalCache(
        factory=lambda: create_markdown_parser(),
        name="markdown_parser",
    )
    
    # Get or create parser for current thread
    parser = parser_cache.get()
    
    # Get parser with a specific key (e.g., engine type)
    mistune_parser = parser_cache.get("mistune")
"""

from __future__ import annotations

import threading
from collections.abc import Callable
from typing import Generic, TypeVar

T = TypeVar("T")


class ThreadLocalCache(Generic[T]):
    """
    Generic thread-local cache with factory pattern.
    
    Creates one instance per thread per key, reusing it for subsequent calls.
    Useful for expensive objects like parsers that are not thread-safe but
    can be reused within a single thread.
    
    Thread Safety:
        Each thread gets its own instance(s), no locking required for access.
        The factory function should be thread-safe if it accesses shared state.
    
    Performance:
        - First access per thread/key: factory() cost (e.g., 10ms for parser)
        - Subsequent access: ~1Âµs (attribute lookup)
    
    Example:
        >>> cache = ThreadLocalCache(lambda: ExpensiveParser(), name="parser")
        >>> parser = cache.get()  # Creates parser for this thread
        >>> parser = cache.get()  # Reuses same parser
    """
    
    def __init__(
        self,
        factory: Callable[[], T] | Callable[[str], T],
        name: str = "default",
    ):
        """
        Initialize thread-local cache.
        
        Args:
            factory: Callable that creates new instances.
                     Can be no-arg or accept a key string.
            name: Name for this cache (used in attribute names)
        """
        self._local = threading.local()
        self._factory = factory
        self._name = name
        self._factory_accepts_key = self._check_factory_signature()
    
    def _check_factory_signature(self) -> bool:
        """Check if factory accepts a key argument."""
        import inspect
        sig = inspect.signature(self._factory)
        params = list(sig.parameters.values())
        return len(params) > 0
    
    def get(self, key: str | None = None) -> T:
        """
        Get or create cached instance for current thread.
        
        Args:
            key: Optional key for multiple instances per thread.
                 Use when caching different variants (e.g., parser engines).
        
        Returns:
            Cached or newly created instance
        """
        cache_key = f"_cache_{self._name}_{key or 'default'}"
        
        if not hasattr(self._local, cache_key):
            if self._factory_accepts_key and key:
                instance = self._factory(key)  # type: ignore
            else:
                instance = self._factory()  # type: ignore
            setattr(self._local, cache_key, instance)
        
        return getattr(self._local, cache_key)
    
    def clear(self, key: str | None = None) -> None:
        """
        Clear cached instance for current thread.
        
        Args:
            key: Specific key to clear, or None to clear default
        """
        cache_key = f"_cache_{self._name}_{key or 'default'}"
        if hasattr(self._local, cache_key):
            delattr(self._local, cache_key)
    
    def clear_all(self) -> None:
        """Clear all cached instances for current thread."""
        # Find all cache keys for this cache name
        to_delete = [
            attr for attr in dir(self._local)
            if attr.startswith(f"_cache_{self._name}_")
        ]
        for attr in to_delete:
            delattr(self._local, attr)


class ThreadSafeSet:
    """
    Thread-safe set for tracking created resources (e.g., directories).
    
    Example:
        >>> created_dirs = ThreadSafeSet()
        >>> if created_dirs.add_if_new("/path/to/dir"):
        ...     os.makedirs("/path/to/dir")  # Only if not already created
    """
    
    def __init__(self):
        self._set: set[str] = set()
        self._lock = threading.Lock()
    
    def add_if_new(self, item: str) -> bool:
        """
        Add item if not present, return True if added.
        
        Thread-safe check-and-add operation.
        
        Args:
            item: Item to add
        
        Returns:
            True if item was new (added), False if already present
        """
        with self._lock:
            if item in self._set:
                return False
            self._set.add(item)
            return True
    
    def __contains__(self, item: str) -> bool:
        with self._lock:
            return item in self._set
    
    def clear(self) -> None:
        with self._lock:
            self._set.clear()
```

---

### Data Flow

1. **Hashing**: Modules import `hash_str()`, `hash_dict()`, `hash_file()` directly
2. **Retry**: Modules wrap operations with `retry_with_backoff()` or `async_retry_with_backoff()`
3. **Thread-local**: Modules create a `ThreadLocalCache` instance at module level, call `.get()` in functions

### Error Handling

- **Hashing**: Raises `FileNotFoundError` for missing files, propagates other I/O errors
- **Retry**: Raises last exception after exhausting retries, provides `on_retry` callback for logging
- **Thread-local**: No new exceptions, factory exceptions propagate to caller

### Configuration

No new configuration options required. Utilities use sensible defaults:
- Hash truncation defaults to `None` (full hash) or `16` for dict hashing
- Retry defaults: 3 retries, 0.5s base delay, 10s max delay
- Thread-local: No configuration needed

### Testing Strategy

1. **Unit tests** for each utility function:
   - `tests/unit/utils/test_hashing.py`
   - `tests/unit/utils/test_retry.py`
   - `tests/unit/utils/test_thread_local.py`

2. **Property-based tests** for hashing determinism:
   - Same input â†’ same output
   - Different input â†’ different output (with high probability)

3. **Concurrency tests** for thread-local:
   - Multiple threads get independent instances
   - No race conditions

4. **Integration tests**: Existing tests continue to pass after migration

---

## Tradeoffs & Risks

### Tradeoffs

| Gain | Cost |
|------|------|
| Single source of truth for hashing | Additional import in consuming modules |
| Consistent retry behavior | Migration effort for existing code |
| Documented thread-local pattern | Learning curve for new contributors |
| Better testability | Initial implementation time |

### Risks

**Risk 1: Migration introduces bugs**
- **Likelihood**: Low
- **Impact**: Medium (could affect caching, builds)
- **Mitigation**: 
  - Incremental migration (one module at a time)
  - Comprehensive tests before migration
  - Keep old code paths initially, switch via feature flag if needed

**Risk 2: Performance regression**
- **Likelihood**: Very Low
- **Impact**: Low (utilities are thin wrappers)
- **Mitigation**: Benchmark critical paths (file hashing, cache lookups)

**Risk 3: Incomplete migration leaves orphaned code**
- **Likelihood**: Medium
- **Impact**: Low (just cleanup debt)
- **Mitigation**: Track migration progress, add deprecation warnings to old functions

---

## Performance & Compatibility

### Performance Impact

- **Build time**: Negligible (utilities are thin wrappers around stdlib)
- **Memory**: No change (thread-local caches same behavior, just centralized)
- **Import time**: +~1ms for new module imports (one-time)

### Compatibility

- **Breaking changes**: None â€” this is internal refactoring
- **Migration path**: N/A for end users; internal only
- **Deprecation timeline**: N/A

---

## Migration & Rollout

### Implementation Phases

**Phase 1: Foundation (Day 1)**
1. Create `utils/hashing.py` with full implementation
2. Create `utils/retry.py` with full implementation
3. Create `utils/thread_local.py` with full implementation
4. Add comprehensive unit tests
5. Update `utils/__init__.py` exports

**Phase 2: Core Migration (Day 2-3)**
1. Migrate `cache/build_cache/` to use `utils/hashing.py`
2. Migrate `rendering/pipeline/thread_local.py` to use `utils/thread_local.py`
3. Migrate `health/linkcheck/async_checker.py` to use `utils/retry.py`
4. Run full test suite

**Phase 3: Extended Migration (Day 4-5)**
1. Migrate remaining hashing uses (core/asset.py, config/hash.py, etc.)
2. Migrate remaining thread-local uses (orchestration, server)
3. Migrate remaining retry uses (utils/file_lock.py, fonts/downloader.py)
4. Remove deprecated inline implementations

**Phase 4: Cleanup & Documentation (Day 6)**
1. Remove orphaned code
2. Update architecture documentation
3. Add migration notes to changelog

### Rollout Strategy

- **Feature flag**: Not needed (internal refactoring)
- **Beta period**: Not needed (covered by test suite)
- **Documentation updates**: 
  - `architecture/utils.md` (if exists, or create)
  - Module docstrings (included in implementation)

---

## Decisions (Resolved Questions)

- **Hashing Performance**: `hash_file()` will use SHA256 initially (stdlib only). Adding `xxhash` support is deferred to Phase 3 as an optional optimization if performance benchmarks justify the dependency.
  
- **Retry Decorators**: Retry utilities will start with function wrappers (`retry_with_backoff`). Decorators can be added later if developer feedback requests them, but explicit wrappers are preferred for transparency.

- **Thread-Local Debugging**: The thread-local cache will include an optional debug mode to log creation counts, aiding in leak detection during development.

---

## Summary

This RFC proposes extracting three utility modules to consolidate repeated patterns:

1. **`utils/hashing.py`** â€” Standardized hashing for 12+ files
2. **`utils/retry.py`** â€” Exponential backoff for 3 files
3. **`utils/thread_local.py`** â€” Thread-local caching for 10 files

The approach is low-risk, follows existing Bengal patterns, and enables incremental migration. Total effort is estimated at 5-6 days with a single developer.

---

## Validation Results

**Audit Date**: 2025-12-08
**Confidence**: 92% (High) ðŸŸ¢

- **Claims Validated**: 4/4
- **Critical Claims**:
  - Hashing duplication (12+ files) verified via code search
  - Thread-local ad-hoc usage verified in core/orchestration
  - `mkdir` duplication confirmed (81+ occurrences)
- **Conclusion**: Proposed design correctly addresses verified pain points with minimal risk.

---

## References

- Evidence from codebase search (2025-12-08)
- Related: `bengal/utils/text.py` (consolidated text utilities)
- Related: `bengal/utils/file_io.py` (consolidated file I/O)
- Architecture: `bengal/architecture/design-principles.md`
