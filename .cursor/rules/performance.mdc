---
globs: bengal/orchestration/**/*.py,bengal/rendering/**/*.py
description: Performance optimization patterns and conventions for Bengal
---

# Performance Optimization for Bengal

Bengal is designed for high performance with parallel processing and smart caching. Follow these patterns when working on performance-critical code.

## Core Principles

### 1. Measure First, Optimize Second

Never optimize without profiling:

```python
# ✅ CORRECT - Profile before optimizing
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()
build_site(site)
profiler.disable()

stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 hot spots
```

### 2. Parallel by Default (with Smart Thresholds)

Use parallel processing for collections >5 items:

```python
from concurrent.futures import ThreadPoolExecutor

# ✅ CORRECT - Parallel with threshold
def render_pages(pages: list[Page], max_workers: int | None = None) -> None:
    """Render pages in parallel if beneficial."""
    PARALLEL_THRESHOLD = 5

    if len(pages) < PARALLEL_THRESHOLD:
        # Sequential for small workloads (avoid thread overhead)
        for page in pages:
            render_page(page)
    else:
        # Parallel for larger workloads
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            list(executor.map(render_page, pages))
```

**Why threshold?** Thread creation overhead (~0.1ms per thread) dominates for small workloads.

See: [architecture/performance.md](mdc:architecture/performance.md)

### 3. Cache Aggressively

Cache expensive operations:

```python
from functools import lru_cache

# ✅ CORRECT - Cache expensive computation
@lru_cache(maxsize=1024)
def parse_markdown(content: str, engine: str = 'mistune') -> str:
    """Parse markdown with caching."""
    return _parse_markdown_uncached(content, engine)

# ✅ CORRECT - File-based cache with hash validation
def get_cached_ast(file_path: Path) -> AST | None:
    """Get cached AST if file unchanged."""
    cache = BuildCache.load()
    current_hash = compute_file_hash(file_path)

    cached_entry = cache.get_ast(str(file_path))
    if cached_entry and cached_entry['hash'] == current_hash:
        return cached_entry['ast']
    return None
```

### 4. Lazy Loading

Parse content only when needed:

```python
# ✅ CORRECT - Lazy loading with caching
class Page:
    _toc: str | None = None

    @property
    def toc(self) -> str:
        """Table of contents (lazy loaded)."""
        if self._toc is None:
            self._toc = extract_toc(self.content)
        return self._toc
```

## Performance Patterns

### ThreadPoolExecutor for I/O-Bound Work

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_files(files: list[Path], max_workers: int | None = None) -> list[Result]:
    """Process files in parallel."""
    results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        futures = {executor.submit(process_file, f): f for f in files}

        # Collect results as they complete
        for future in as_completed(futures):
            file_path = futures[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")

    return results
```

### Batch Operations

```python
# ✅ CORRECT - Batch file operations
def write_pages(pages: list[Page], output_dir: Path) -> None:
    """Write multiple pages efficiently."""
    # Group by directory
    by_dir: dict[Path, list[Page]] = {}
    for page in pages:
        dir_path = output_dir / page.section_path
        by_dir.setdefault(dir_path, []).append(page)

    # Create all directories at once
    for dir_path in by_dir:
        dir_path.mkdir(parents=True, exist_ok=True)

    # Write files in parallel
    with ThreadPoolExecutor() as executor:
        for dir_path, dir_pages in by_dir.items():
            for page in dir_pages:
                executor.submit(write_page, page, dir_path)
```

### Incremental Processing

```python
# ✅ CORRECT - Skip unchanged files
def build_incremental(site: Site) -> None:
    """Build only changed pages."""
    cache = BuildCache.load()

    changed_pages = []
    for page in site.pages:
        current_hash = compute_file_hash(page.source_path)
        cached_hash = cache.get_file_hash(str(page.source_path))

        if current_hash != cached_hash:
            changed_pages.append(page)
            cache.update_hash(str(page.source_path), current_hash)

    logger.info(f"Building {len(changed_pages)}/{len(site.pages)} changed pages")
    render_pages(changed_pages)
    cache.save()
```

## Performance Benchmarks

Document performance characteristics:

```python
def render_pages_parallel(pages: list[Page]) -> None:
    """
    Render pages using parallel processing.

    Performance:
        - 10 pages: 2.3s (sequential: 2.5s, 9% slower due to overhead)
        - 100 pages: 8.2s (sequential: 15.4s, 1.9x speedup)
        - 1000 pages: 45.1s (sequential: 168.3s, 3.7x speedup)

    Tested on: 8-core M1 Pro, mistune parser, default theme
    See: benchmarks/test_build.py for full results
    """
    # Implementation
    pass
```

## Hot Path Optimization

### Markdown Parsing (40-50% of build time)

```python
# ✅ CORRECT - Use fastest parser for production
PARSER_CHOICE = {
    'development': 'mistune',  # Fast, good enough
    'production': 'mistune',   # Fast, well-tested
}

# Profile different parsers
# mistune: 100 pages in 1.2s
# python-markdown: 100 pages in 3.8s (3.2x slower)
```

### Template Rendering (30-40% of build time)

```python
# ✅ CORRECT - Cache compiled templates
from jinja2 import Environment, FileSystemLoader, select_autoescape

env = Environment(
    loader=FileSystemLoader(template_dir),
    autoescape=select_autoescape(['html', 'xml']),
    cache_size=400,  # Cache compiled templates
)
```

### File I/O (10-20% of build time)

```python
# ✅ CORRECT - Batch reads, async writes
from pathlib import Path

# Read files efficiently
content = Path(file_path).read_text(encoding='utf-8')

# Write with buffering
with open(output_path, 'w', encoding='utf-8', buffering=8192) as f:
    f.write(html)
```

## Memory Optimization

### Avoid Loading Everything at Once

```python
# ✅ CORRECT - Generator for large collections
def iter_content_files(content_dir: Path) -> Iterator[Path]:
    """Iterate over content files without loading all at once."""
    for file_path in content_dir.rglob("*.md"):
        if not file_path.name.startswith('_'):
            yield file_path

# Use generator
for file_path in iter_content_files(content_dir):
    process_file(file_path)

# ❌ WRONG - Loads all files into memory
files = list(content_dir.rglob("*.md"))
```

### Release Large Objects Early

```python
# ✅ CORRECT - Clear references when done
def build_site(site: Site) -> None:
    """Build site and cleanup."""
    pages = discover_pages(site)
    render_pages(pages)

    # Clear large objects
    del pages
    gc.collect()  # Force cleanup if needed
```

## Profiling Tools

### Built-in cProfile

```python
import cProfile
import pstats

# Profile function
profiler = cProfile.Profile()
profiler.enable()
my_function()
profiler.disable()

# Analyze results
stats = pstats.Stats(profiler)
stats.strip_dirs()
stats.sort_stats('cumulative')
stats.print_stats(20)
```

### Memory Profiling

```python
from memory_profiler import profile

@profile
def memory_intensive_function():
    """Function with memory profiling."""
    data = [0] * 10000000
    return sum(data)
```

### Line Profiling

```bash
# Install line_profiler
pip install line_profiler

# Add @profile decorator
@profile
def hot_function():
    # Line-by-line profiling
    pass

# Run with kernprof
kernprof -l -v script.py
```

## Performance Configuration

Make performance tunable via config:

```yaml
# config/_default/build.yaml
build:
  parallel: true           # Enable parallel processing
  max_workers: null        # Auto-detect CPU count
  incremental: true        # Enable incremental builds

  # Performance thresholds
  parallel_threshold: 5    # Minimum items for parallel processing
  cache_size: 1000         # Template cache size
```

## Performance Tests

Write performance tests for critical paths:

```python
import pytest
import time

@pytest.mark.slow
def test_large_site_build_performance(site_factory):
    """Test build performance on large site."""
    site = site_factory("test-large")  # 1000 pages

    start = time.perf_counter()
    site.build()
    duration = time.perf_counter() - start

    # Assert performance threshold
    assert duration < 60.0, f"Build took {duration:.2f}s, expected <60s"

    # Log for benchmarking
    print(f"Large site build: {duration:.2f}s ({len(site.pages)} pages)")
```

## Performance Monitoring

Add timing logs for key operations:

```python
import time
import logging

def render_pages(pages: list[Page]) -> None:
    """Render pages with timing."""
    start = time.perf_counter()

    # Do work
    for page in pages:
        render_page(page)

    duration = time.perf_counter() - start
    pages_per_sec = len(pages) / duration if duration > 0 else 0

    logger.info(
        f"Rendered {len(pages)} pages in {duration:.2f}s "
        f"({pages_per_sec:.1f} pages/sec)"
    )
```

## Future Optimizations

Document potential optimizations:

```python
# TODO: Optimize for large sites (>10k pages)
# Current: O(n²) for link validation
# Proposed: Use trie for O(n log n) validation
# Expected speedup: 10x for 10k pages
# Priority: Medium (affects <1% of users)
```

## Performance Documentation

- [architecture/performance.md](mdc:architecture/performance.md) - Performance architecture
- [benchmarks/](mdc:benchmarks/) - Performance benchmarks
- [benchmarks/README.md](mdc:benchmarks/README.md) - Benchmark documentation

## Key Takeaways

1. **Profile first** - Measure before optimizing
2. **Parallel > 5 items** - Use ThreadPoolExecutor for collections >5
3. **Cache everything** - File hashes, parsed AST, templates
4. **Lazy load** - Parse content only when needed
5. **Document benchmarks** - Include performance characteristics
6. **Test performance** - Write performance regression tests
