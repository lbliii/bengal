"""Perl lexer for Rosettes.

Thread-safe regex-based tokenizer for Perl source code.
"""

import re

from bengal.rendering.rosettes._types import TokenType
from bengal.rendering.rosettes.lexers._base import PatternLexer, Rule

__all__ = ["PerlLexer"]

_KEYWORDS = (
    "BEGIN",
    "CHECK",
    "END",
    "INIT",
    "UNITCHECK",
    "break",
    "continue",
    "default",
    "do",
    "else",
    "elsif",
    "for",
    "foreach",
    "given",
    "goto",
    "if",
    "last",
    "local",
    "my",
    "next",
    "our",
    "package",
    "redo",
    "require",
    "return",
    "state",
    "sub",
    "unless",
    "until",
    "use",
    "when",
    "while",
)

_BUILTINS = (
    "abs",
    "accept",
    "alarm",
    "atan2",
    "bind",
    "binmode",
    "bless",
    "caller",
    "chdir",
    "chmod",
    "chomp",
    "chop",
    "chown",
    "chr",
    "chroot",
    "close",
    "closedir",
    "connect",
    "cos",
    "crypt",
    "dbmclose",
    "dbmopen",
    "defined",
    "delete",
    "die",
    "dump",
    "each",
    "endgrent",
    "endhostent",
    "endnetent",
    "endprotoent",
    "endpwent",
    "endservent",
    "eof",
    "eval",
    "exec",
    "exists",
    "exit",
    "exp",
    "fcntl",
    "fileno",
    "flock",
    "fork",
    "format",
    "formline",
    "getc",
    "getgrent",
    "getgrgid",
    "getgrnam",
    "gethostbyaddr",
    "gethostbyname",
    "gethostent",
    "getlogin",
    "getnetbyaddr",
    "getnetbyname",
    "getnetent",
    "getpeername",
    "getpgrp",
    "getppid",
    "getpriority",
    "getprotobyname",
    "getprotobynumber",
    "getprotoent",
    "getpwent",
    "getpwnam",
    "getpwuid",
    "getservbyname",
    "getservbyport",
    "getservent",
    "getsockname",
    "getsockopt",
    "glob",
    "gmtime",
    "grep",
    "hex",
    "import",
    "index",
    "int",
    "ioctl",
    "join",
    "keys",
    "kill",
    "lc",
    "lcfirst",
    "length",
    "link",
    "listen",
    "localtime",
    "log",
    "lstat",
    "map",
    "mkdir",
    "msgctl",
    "msgget",
    "msgrcv",
    "msgsnd",
    "oct",
    "open",
    "opendir",
    "ord",
    "pack",
    "pipe",
    "pop",
    "pos",
    "print",
    "printf",
    "prototype",
    "push",
    "quotemeta",
    "rand",
    "read",
    "readdir",
    "readline",
    "readlink",
    "readpipe",
    "recv",
    "ref",
    "rename",
    "reset",
    "reverse",
    "rewinddir",
    "rindex",
    "rmdir",
    "say",
    "scalar",
    "seek",
    "seekdir",
    "select",
    "semctl",
    "semget",
    "semop",
    "send",
    "setgrent",
    "sethostent",
    "setnetent",
    "setpgrp",
    "setpriority",
    "setprotoent",
    "setpwent",
    "setservent",
    "setsockopt",
    "shift",
    "shmctl",
    "shmget",
    "shmread",
    "shmwrite",
    "shutdown",
    "sin",
    "sleep",
    "socket",
    "socketpair",
    "sort",
    "splice",
    "split",
    "sprintf",
    "sqrt",
    "srand",
    "stat",
    "study",
    "substr",
    "symlink",
    "syscall",
    "sysopen",
    "sysread",
    "sysseek",
    "system",
    "syswrite",
    "tell",
    "telldir",
    "tie",
    "tied",
    "time",
    "times",
    "truncate",
    "uc",
    "ucfirst",
    "umask",
    "undef",
    "unlink",
    "unpack",
    "unshift",
    "untie",
    "values",
    "vec",
    "wait",
    "waitpid",
    "wantarray",
    "warn",
    "write",
)


def _classify_word(match: re.Match[str]) -> TokenType:
    word = match.group(0)
    if word in ("sub", "my", "our", "local", "state"):
        return TokenType.KEYWORD_DECLARATION
    if word in ("use", "require", "package"):
        return TokenType.KEYWORD_NAMESPACE
    if word in _KEYWORDS:
        return TokenType.KEYWORD
    if word in _BUILTINS:
        return TokenType.NAME_BUILTIN
    return TokenType.NAME


class PerlLexer(PatternLexer):
    """Perl lexer. Thread-safe."""

    name = "perl"
    aliases = ("pl", "pm")
    filenames = ("*.pl", "*.pm", "*.t")
    mimetypes = ("text/x-perl", "application/x-perl")

    _WORD_PATTERN = r"\b[a-zA-Z_][a-zA-Z0-9_]*\b"

    rules = (
        # POD documentation
        Rule(re.compile(r"^=\w+[\s\S]*?^=cut", re.MULTILINE), TokenType.STRING_DOC),
        # Comments
        Rule(re.compile(r"#.*$", re.MULTILINE), TokenType.COMMENT_SINGLE),
        # Heredocs (simplified)
        Rule(re.compile(r"<<'(\w+)'.*?\n[\s\S]*?\n\1"), TokenType.STRING_HEREDOC),
        Rule(re.compile(r'<<"(\w+)".*?\n[\s\S]*?\n\1'), TokenType.STRING_HEREDOC),
        Rule(re.compile(r"<<(\w+).*?\n[\s\S]*?\n\1"), TokenType.STRING_HEREDOC),
        # Regex
        Rule(
            re.compile(r"(?:qr|m|s|tr|y)/(?:[^/\\]|\\.)+/(?:[^/\\]|\\.)*(?:/[msixpodualngc]*)?"),
            TokenType.STRING_REGEX,
        ),
        # Quote-like operators
        Rule(re.compile(r"q[qwx]?\{[^}]*\}"), TokenType.STRING),
        Rule(re.compile(r"q[qwx]?\[[^\]]*\]"), TokenType.STRING),
        Rule(re.compile(r"q[qwx]?\([^)]*\)"), TokenType.STRING),
        # Strings
        Rule(re.compile(r'"[^"\\]*(?:\\.[^"\\]*)*"'), TokenType.STRING_DOUBLE),
        Rule(re.compile(r"'[^'\\]*(?:\\.[^'\\]*)*'"), TokenType.STRING_SINGLE),
        # Variables
        Rule(
            re.compile(r"\$[a-zA-Z_][a-zA-Z0-9_]*(?:::[a-zA-Z_][a-zA-Z0-9_]*)*"),
            TokenType.NAME_VARIABLE,
        ),
        Rule(re.compile(r"@[a-zA-Z_][a-zA-Z0-9_]*"), TokenType.NAME_VARIABLE),
        Rule(re.compile(r"%[a-zA-Z_][a-zA-Z0-9_]*"), TokenType.NAME_VARIABLE),
        Rule(re.compile(r"\$[{]?[0-9]+[}]?"), TokenType.NAME_VARIABLE),
        Rule(re.compile(r"\$[{]?[!@#$%^&*_]+[}]?"), TokenType.NAME_VARIABLE_MAGIC),
        # Numbers
        Rule(re.compile(r"0[xX][0-9a-fA-F_]+"), TokenType.NUMBER_HEX),
        Rule(re.compile(r"0[bB][01_]+"), TokenType.NUMBER_BIN),
        Rule(re.compile(r"0[0-7_]+"), TokenType.NUMBER_OCT),
        Rule(re.compile(r"\d[\d_]*\.\d[\d_]*(?:[eE][+-]?\d[\d_]*)?"), TokenType.NUMBER_FLOAT),
        Rule(re.compile(r"\d[\d_]*[eE][+-]?\d[\d_]*"), TokenType.NUMBER_FLOAT),
        Rule(re.compile(r"\d[\d_]*"), TokenType.NUMBER_INTEGER),
        # Keywords/names
        Rule(re.compile(_WORD_PATTERN), _classify_word),
        # Operators
        Rule(re.compile(r"->|=>|=~|!~|~~|\.\.|\.\.\."), TokenType.OPERATOR),
        Rule(re.compile(r"&&|\|\||//|==|!=|<=>|<=|>=|<<|>>"), TokenType.OPERATOR),
        Rule(re.compile(r"\+\+|--|\*\*"), TokenType.OPERATOR),
        Rule(re.compile(r"[+\-*/%&|^!~<>=?:]"), TokenType.OPERATOR),
        # Punctuation
        Rule(re.compile(r"[()[\]{}:;,.]"), TokenType.PUNCTUATION),
        # Whitespace
        Rule(re.compile(r"\s+"), TokenType.WHITESPACE),
    )
