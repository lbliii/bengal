
---
title: "benchmark_history"
type: "python-module"
source_file: "benchmarks/benchmark_history.py"
line_number: 1
description: "Benchmark history tracking with append-only log. Automatically captures benchmark results with timestamps for measuring performance across patches, commits, and releases."
---

# benchmark_history
**Type:** Module
**Source:** [View source](benchmarks/benchmark_history.py#L1)



**Navigation:**
[benchmarks](/api/benchmarks/) â€ºbenchmark_history

Benchmark history tracking with append-only log.

Automatically captures benchmark results with timestamps for measuring
performance across patches, commits, and releases.

## Classes




### `BenchmarkHistoryLogger`


Append-only benchmark history log with timestamps and metadata.









## Methods



#### `__init__`

:::{div} api-badge-group
:::

```python
def __init__(self, log_file: Path | None = None)
```


Initialize history logger.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `log_file` | `Path \| None` | - | Path to append-only log file. Defaults to .benchmarks/history.jsonl |








#### `log_run`

:::{div} api-badge-group
:::

```python
def log_run(self, results: dict[str, Any], git_commit: str | None = None, git_branch: str | None = None, version: str | None = None, metadata: dict[str, Any] | None = None) -> None
```


Log a benchmark run with timestamp and metadata.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `results` | `dict[str, Any]` | - | Dictionary of benchmark results from pytest-benchmark |
| `git_commit` | `str \| None` | - | Git commit hash (optional) |
| `git_branch` | `str \| None` | - | Git branch name (optional) |
| `version` | `str \| None` | - | Application version (optional) |
| `metadata` | `dict[str, Any] \| None` | - | Additional metadata to track (optional) |







:::{rubric} Returns
:class: rubric-returns
:::


`None`



#### `get_history`

:::{div} api-badge-group
:::

```python
def get_history(self, limit: int | None = None) -> list[dict[str, Any]]
```


Read benchmark history.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `limit` | `int \| None` | - | Maximum number of recent entries to return (None = all) |







:::{rubric} Returns
:class: rubric-returns
:::


`list[dict[str, Any]]` - List of benchmark history entries, most recent last.



#### `get_latest_results`

:::{div} api-badge-group
:::

```python
def get_latest_results(self) -> dict[str, Any] | None
```


Get the most recent benchmark run.



:::{rubric} Returns
:class: rubric-returns
:::


`dict[str, Any] | None`



#### `export_csv`

:::{div} api-badge-group
:::

```python
def export_csv(self, output_file: Path, metric: str = 'mean') -> None
```


Export benchmark history as CSV for analysis and graphing.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `output_file` | `Path` | - | Path to write CSV file |
| `metric` | `str` | `'mean'` | Metric to extract (mean, min, max, stddev, etc) |







:::{rubric} Returns
:class: rubric-returns
:::


`None`



#### `print_summary`

:::{div} api-badge-group
:::

```python
def print_summary(self, last_n: int = 5) -> None
```


Print a summary of recent benchmark runs.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `last_n` | `int` | `5` | Number of recent runs to show |







:::{rubric} Returns
:class: rubric-returns
:::


`None`



---
*Generated by Bengal autodoc from `benchmarks/benchmark_history.py`*

