
---
title: "benchmark_builds"
type: "python-module"
source_file: "scripts/benchmark_builds.py"
line_number: 1
description: "Benchmark different Bengal build modes for cold builds. Compares: 1. Standard build (parallel) 2. Standard build (sequential / --no-parallel) 3. Fast mode (--fast) Each run: - Cleans cache completely ..."
---

# benchmark_builds
**Type:** Module
**Source:** [View source](scripts/benchmark_builds.py#L1)



**Navigation:**
[scripts](/api/scripts/) â€ºbenchmark_builds

Benchmark different Bengal build modes for cold builds.

Compares:
1. Standard build (parallel)
2. Standard build (sequential / --no-parallel)
3. Fast mode (--fast)

Each run:
- Cleans cache completely (ensures cold build)
- Times the build
- Captures key metrics

## Classes




### `BuildResult`


Result of a single build run.


:::{info}
This is a dataclass.
:::



**Attributes:**

:::{div} api-attributes
`name`
: 

`command`
: 

`elapsed_seconds`
: 

`exit_code`
: 

`success`
: 

`pages_rendered`
: 

`output_lines`
: 

`error_output`
: 

:::







## Methods



#### `__str__`

:::{div} api-badge-group
:::

```python
def __str__(self) -> str
```


*No description provided.*



:::{rubric} Returns
:class: rubric-returns
:::


`str`

## Functions



### `clean_cache`


```python
def clean_cache(site_dir: Path) -> None
```



Clean cache and output directories for a cold build.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `site_dir` | `Path` | - | *No description provided.* |







**Returns**


`None`




### `run_build`


```python
def run_build(site_dir: Path, name: str, *args: str) -> BuildResult
```



Run a build with given arguments and measure time.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `site_dir` | `Path` | - | *No description provided.* |
| `name` | `str` | - | *No description provided.* |







**Returns**


`BuildResult`




### `run_benchmarks`


```python
def run_benchmarks(site_dir: Path, iterations: int = 1) -> list[BuildResult]
```



Run all benchmark configurations.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `site_dir` | `Path` | - | *No description provided.* |
| `iterations` | `int` | `1` | *No description provided.* |







**Returns**


`list[BuildResult]`




### `print_summary`


```python
def print_summary(results: list[BuildResult]) -> None
```



Print a summary comparison of all builds.


**Parameters:**

| Name | Type | Default | Description |
|:-----|:-----|:--------|:------------|
| `results` | `list[BuildResult]` | - | *No description provided.* |







**Returns**


`None`




### `main`


```python
def main() -> None
```



Main entry point.



**Returns**


`None`



---
*Generated by Bengal autodoc from `scripts/benchmark_builds.py`*

